{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- standard libs ---\n",
        "import math, re, sys\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from datetime import datetime, timezone\n",
        "# --- light scraping deps (pip install if missing) ---\n",
        "# pip install requests beautifulsoup4\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# =========================\n",
        "# SCORING (unchanged logic)\n",
        "# =========================\n",
        "def score_articles(articles, weights=None):\n",
        "    \"\"\"\n",
        "    Score a list of article dicts for credibility & objectivity.\n",
        "\n",
        "    Each article can include ANY of these (all optional):\n",
        "      title, url, domain, outlet_type, published_at, updated_at,\n",
        "      has_byline, has_about_page, has_contact_info, corrections_policy,\n",
        "      cites_sources_count, links_to_primary_sources, quotes_named_experts,\n",
        "      advertising_density, uses_stock_images_only, is_press_release,\n",
        "      clickbait_score, subjectivity_score, polarity_score, paywalled\n",
        "\n",
        "    Returns list of dicts with:\n",
        "      credibility_score_0_100, objectivity_stars_0_5, overall_stars_0_5,\n",
        "      rationale, breakdown\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------- Weighting for rule-based components ----------\n",
        "    W = {\n",
        "        \"domain\": 0.23,        # Trustworthiness of the domain / outlet type\n",
        "        \"evidence\": 0.22,      # Citations, primary sources, expert quotes\n",
        "        \"transparency\": 0.18,  # Presence of bylines, about pages, contact info\n",
        "        \"quality\": 0.15,       # Article quality signals (ads density, stock images, PR)\n",
        "        \"recency\": 0.12,       # How recent/up-to-date the article is\n",
        "        \"objectivity\": 0.10,   # Subjectivity, sentiment, and opinion cues\n",
        "    }\n",
        "    if weights:\n",
        "        W.update(weights)\n",
        "\n",
        "    TRUSTED_TLDS = (\".gov\", \".edu\")\n",
        "    SUSPECT_TLDS  = (\".zip\", \".top\", \".click\", \".work\", \".country\", \".gq\", \".cf\", \".ml\")\n",
        "\n",
        "    CLICKBAIT_PATTERNS = [\n",
        "        r\"\\bshocking\\b\", r\"\\byou won'?t believe\\b\", r\"\\bwhat happened next\\b\",\n",
        "        r\"\\b(exposed|leaked)\\b\", r\"\\b(slam|destroys|obliterates)\\b\", r\"\\b(OMG|wow)\\b\",\n",
        "        r\"\\bsecret(s)?\\b\", r\"\\bno one is talking about\\b\"\n",
        "    ]\n",
        "    OPINION_CUES = [r\"^opinion\\b\", r\"^editorial\\b\", r\"\\bop-ed\\b\", r\"\\bcommentary\\b\"]\n",
        "\n",
        "    def _safe_lower(s):\n",
        "        return (s or \"\").strip().lower()\n",
        "\n",
        "    def _domain(url, domain):\n",
        "        if domain: return domain.lower()\n",
        "        if url:\n",
        "            try: return urlparse(url).netloc.lower()\n",
        "            except Exception: return \"\"\n",
        "        return \"\"\n",
        "\n",
        "    def _parse_date(s):\n",
        "        if not s: return None\n",
        "        try:\n",
        "            dt = datetime.fromisoformat(s.replace(\"Z\", \"+00:00\"))\n",
        "            if not dt.tzinfo: dt = dt.replace(tzinfo=timezone.utc)\n",
        "            return dt\n",
        "        except Exception:\n",
        "            pass\n",
        "        for fmt in (\"%Y-%m-%d\", \"%Y/%m/%d\", \"%m/%d/%Y\"):\n",
        "            try: return datetime.strptime(s, fmt).replace(tzinfo=timezone.utc)\n",
        "            except Exception: continue\n",
        "        return None\n",
        "\n",
        "    def _years_since(dt):\n",
        "        if not dt: return None\n",
        "        now = datetime.now(timezone.utc)\n",
        "        return (now - dt).days / 365.25\n",
        "\n",
        "    # ---------- Component scoring ----------\n",
        "    def score_domain(url, domain, outlet_type):\n",
        "        d = _domain(url, domain)\n",
        "        rep = 0.5\n",
        "        if any(d.endswith(t) for t in TRUSTED_TLDS): rep = 0.85\n",
        "        if any(d.endswith(t) for t in SUSPECT_TLDS): rep = min(rep, 0.35)\n",
        "        t = _safe_lower(outlet_type)\n",
        "        if t in (\"university\", \"gov\", \"government\", \"journal\"): rep = max(rep, 0.9)\n",
        "        elif t in (\"newspaper\", \"magazine\", \"wire\", \"public broadcaster\"): rep = max(rep, 0.75)\n",
        "        elif t in (\"blog\", \"substack\", \"youtube\", \"social\"): rep = min(rep, 0.55)\n",
        "        return rep\n",
        "\n",
        "    def score_recency(published_at, updated_at):\n",
        "        pts = 0.5\n",
        "        base = _parse_date(published_at) or _parse_date(updated_at)\n",
        "        if base:\n",
        "            y = _years_since(base) or 100\n",
        "            if y <= (30/365.25): pts = 1.0\n",
        "            elif y <= 0.5: pts = 0.85\n",
        "            elif y <= 2: pts = 0.7\n",
        "            elif y <= 5: pts = 0.5\n",
        "            else: pts = 0.3\n",
        "        return pts\n",
        "\n",
        "    def score_transparency(x):\n",
        "        pts = 0.3\n",
        "        if x.get(\"has_byline\") is True: pts += 0.25\n",
        "        if x.get(\"has_about_page\") is True: pts += 0.2\n",
        "        if x.get(\"has_contact_info\") is True: pts += 0.15\n",
        "        if x.get(\"corrections_policy\") is True: pts += 0.15\n",
        "        return max(0.0, min(1.0, pts))\n",
        "\n",
        "    def score_evidence(x):\n",
        "        cites = x.get(\"cites_sources_count\") or 0\n",
        "        primary = x.get(\"links_to_primary_sources\") or 0\n",
        "        experts = x.get(\"quotes_named_experts\") or 0\n",
        "        cites_pts = 1 - math.exp(-cites/4)\n",
        "        prim_pts  = 1 - math.exp(-primary/2)\n",
        "        exp_pts   = 1 - math.exp(-experts/2)\n",
        "        return (0.45*cites_pts + 0.35*prim_pts + 0.20*exp_pts)\n",
        "\n",
        "    def score_quality(x):\n",
        "        pts = 0.7\n",
        "        ad = x.get(\"advertising_density\")\n",
        "        if ad is not None: pts -= max(0.0, min(0.5, float(ad) * 0.6))\n",
        "        if x.get(\"uses_stock_images_only\") is True: pts -= 0.1\n",
        "        if x.get(\"is_press_release\") is True: pts -= 0.15\n",
        "        return max(0.0, min(1.0, pts))\n",
        "\n",
        "    def score_objectivity(x):\n",
        "        base = 0.7\n",
        "        title_l = _safe_lower(x.get(\"title\"))\n",
        "        if any(re.search(p, title_l) for p in OPINION_CUES): base -= 0.2\n",
        "        subj = x.get(\"subjectivity_score\")  # optional\n",
        "        pol  = x.get(\"polarity_score\")      # optional\n",
        "        if subj is not None: base += 0.25 * (1 - max(0.0, min(1.0, float(subj))))\n",
        "        if pol is not None:  base += 0.15 * (1 - min(1.0, abs(float(pol))))\n",
        "        return max(0.0, min(1.0, base))\n",
        "\n",
        "    def penalties(x):\n",
        "        p = 0.0\n",
        "        title_l = _safe_lower(x.get(\"title\"))\n",
        "        for pat in CLICKBAIT_PATTERNS:\n",
        "            if re.search(pat, title_l): p += 0.08\n",
        "        cb = x.get(\"clickbait_score\")\n",
        "        if cb is not None: p += 0.15 * max(0.0, min(1.0, float(cb)))\n",
        "        if x.get(\"paywalled\") is True: p += 0.03\n",
        "        return min(0.5, p)\n",
        "\n",
        "    def stars_from_objectivity(obj_0_1, evid_0_1, pen_0_1):\n",
        "        obj = max(0.0, min(1.0, obj_0_1 * 0.8 + 0.2*evid_0_1 - 0.3*pen_0_1))\n",
        "        return round(obj * 5, 2)\n",
        "\n",
        "    out = []\n",
        "    for a in articles:\n",
        "        domain = score_domain(a.get(\"url\"), a.get(\"domain\"), a.get(\"outlet_type\"))\n",
        "        rec = score_recency(a.get(\"published_at\"), a.get(\"updated_at\"))\n",
        "        transp = score_transparency(a)\n",
        "        evid = score_evidence(a)\n",
        "        qual = score_quality(a)\n",
        "        obj = score_objectivity(a)\n",
        "        pen = penalties(a)\n",
        "\n",
        "        cred_0_1 = (\n",
        "            W[\"domain\"]*domain + W[\"evidence\"]*evid + W[\"transparency\"]*transp +\n",
        "            W[\"quality\"]*qual + W[\"recency\"]*rec + W[\"objectivity\"]*obj\n",
        "        )\n",
        "        cred_0_1 = max(0.0, min(1.0, cred_0_1 - pen))\n",
        "\n",
        "        credibility_score_0_100 = round(cred_0_1 * 100, 1)\n",
        "        objectivity_stars_0_5 = stars_from_objectivity(obj, evid, pen)\n",
        "        overall_0_1 = max(0.0, min(1.0, 0.7*cred_0_1 + 0.3*(objectivity_stars_0_5/5.0)))\n",
        "        overall_stars_0_5 = round(overall_0_1 * 5, 2)\n",
        "\n",
        "        rationale = (\n",
        "            f\"domain={domain:.2f}, evidence={evid:.2f}, transparency={transp:.2f}, \"\n",
        "            f\"quality={qual:.2f}, recency={rec:.2f}, objectivity={obj:.2f}, penalties={pen:.2f}.\"\n",
        "        )\n",
        "\n",
        "        out.append({\n",
        "            \"title\": a.get(\"title\"),\n",
        "            \"url\": a.get(\"url\"),\n",
        "            \"credibility_score_0_100\": credibility_score_0_100,\n",
        "            \"objectivity_stars_0_5\": objectivity_stars_0_5,\n",
        "            \"overall_stars_0_5\": overall_stars_0_5,\n",
        "            \"rationale\": rationale,\n",
        "            \"breakdown\": {\n",
        "                \"domain\": round(domain, 2),\n",
        "                \"evidence\": round(evid, 2),\n",
        "                \"transparency\": round(transp, 2),\n",
        "                \"quality\": round(qual, 2),\n",
        "                \"recency\": round(rec, 2),\n",
        "                \"objectivity\": round(obj, 2),\n",
        "                \"penalties\": round(pen, 2),\n",
        "            }\n",
        "        })\n",
        "    return out\n",
        "\n",
        "# ==========================================\n",
        "# URL → FEATURES (auto-extraction from HTML)\n",
        "# ==========================================\n",
        "def _fetch_html(url):\n",
        "    \"\"\"Fetch HTML with a safe User-Agent and short timeout.\"\"\"\n",
        "    resp = requests.get(url, timeout=12, headers={\"User-Agent\": \"Mozilla/5.0 (cred-bot)\"})\n",
        "    resp.raise_for_status()\n",
        "    return resp.text\n",
        "\n",
        "def _text_or_attr(tag, *attrs):\n",
        "    \"\"\"Get text or attribute from a BeautifulSoup tag, safely.\"\"\"\n",
        "    if not tag: return None\n",
        "    for a in attrs:\n",
        "        if tag.has_attr(a):\n",
        "            val = tag.get(a)\n",
        "            if val: return val.strip()\n",
        "    # Fallback to tag text\n",
        "    txt = getattr(tag, \"text\", None)\n",
        "    return txt.strip() if txt else None\n",
        "\n",
        "def _guess_outlet_type(domain):\n",
        "    \"\"\"Rough outlet type guess from domain suffix.\"\"\"\n",
        "    d = domain.lower()\n",
        "    if d.endswith(\".edu\"): return \"university\"\n",
        "    if d.endswith(\".gov\"): return \"gov\"\n",
        "    return None  # let scorer treat as neutral\n",
        "\n",
        "def _count_outbound_links(soup, page_domain):\n",
        "    \"\"\"Count outbound links as a proxy for citations/evidence.\"\"\"\n",
        "    cnt = 0\n",
        "    primary = 0\n",
        "    primary_patterns = (\".gov\", \".edu\", \"doi.org\", \"pubmed.\", \"nature.com\", \"science.org\")\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"].strip()\n",
        "        # Normalize absolute\n",
        "        n = urlparse(href)\n",
        "        if not n.netloc:\n",
        "            # relative link → same site (not outbound)\n",
        "            continue\n",
        "        # Outbound if domain differs\n",
        "        if n.netloc and page_domain not in n.netloc:\n",
        "            cnt += 1\n",
        "            if any(p in href.lower() for p in primary_patterns):\n",
        "                primary += 1\n",
        "    return cnt, primary\n",
        "\n",
        "def _detect_byline(soup):\n",
        "    \"\"\"Detect byline via common meta tags / classes.\"\"\"\n",
        "    if soup.find(attrs={\"name\": \"author\"}): return True\n",
        "    if soup.find(\"meta\", {\"property\": \"article:author\"}): return True\n",
        "    if soup.find(class_=re.compile(r\"byline|author\", re.I)): return True\n",
        "    return False\n",
        "\n",
        "def _detect_contact_info(soup):\n",
        "    \"\"\"Crudely detect presence of contact info on page or footer links.\"\"\"\n",
        "    footer = soup.find(\"footer\")\n",
        "    hay = (footer or soup).get_text(\" \", strip=True).lower()\n",
        "    return any(k in hay for k in [\"contact\", \"email\", \"feedback\", \"editor@\", \"newsroom\"])\n",
        "\n",
        "def _detect_about_page(soup):\n",
        "    \"\"\"Look for an About link in header/footer/nav.\"\"\"\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        txt = (a.get_text() or \"\").strip().lower()\n",
        "        if \"about\" in txt or \"masthead\" in txt:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def _detect_corrections_policy(soup):\n",
        "    hay = soup.get_text(\" \", strip=True).lower()\n",
        "    return any(k in hay for k in [\"correction\", \"corrections policy\", \"errata\", \"clarification\"])\n",
        "\n",
        "def _detect_press_release(soup):\n",
        "    hay = soup.get_text(\" \", strip=True).lower()\n",
        "    return (\"press release\" in hay) or (\"newswire\" in hay)\n",
        "\n",
        "def _estimate_ads_density(soup):\n",
        "    \"\"\"Very rough: count iframes & common ad class hints normalized by length.\"\"\"\n",
        "    html_len = len(soup.get_text(\" \", strip=True)) + 1\n",
        "    ad_iframes = len(soup.find_all(\"iframe\"))\n",
        "    ad_divs = len(soup.find_all(class_=re.compile(r\"advert|ad-container|sponsor\", re.I)))\n",
        "    score = min(1.0, (ad_iframes*2 + ad_divs) / max(800, html_len/5))\n",
        "    return round(score, 3)\n",
        "\n",
        "def _detect_paywall(soup):\n",
        "    \"\"\"Simple: look for common paywall flags.\"\"\"\n",
        "    txt = soup.get_text(\" \", strip=True).lower()\n",
        "    flags = [\"subscribe to read\", \"subscriber-only\", \"meteredpaywall\", \"paywall\", \"sign in to continue\"]\n",
        "    return any(f in txt for f in flags)\n",
        "\n",
        "def _extract_date(soup):\n",
        "    \"\"\"Try several standard meta/time locations for published/updated dates.\"\"\"\n",
        "    # OpenGraph / schema.org metas\n",
        "    meta_keys = [\n",
        "        (\"meta\", {\"property\": \"article:published_time\"}),\n",
        "        (\"meta\", {\"name\": \"datePublished\"}),\n",
        "        (\"meta\", {\"itemprop\": \"datePublished\"}),\n",
        "        (\"time\", {\"itemprop\": \"datePublished\"}),\n",
        "        (\"time\", {\"datetime\": True}),\n",
        "    ]\n",
        "    for tag, attrs in meta_keys:\n",
        "        t = soup.find(tag, attrs)\n",
        "        v = _text_or_attr(t, \"content\", \"datetime\")\n",
        "        if v: return v\n",
        "    return None\n",
        "\n",
        "def _count_named_experts(text):\n",
        "    \"\"\"\n",
        "    Heuristic: count quotes that include expert indicators like Dr./Prof./PhD/MD.\n",
        "    Not perfect, but gives a weak signal.\n",
        "    \"\"\"\n",
        "    quotes = re.findall(r\"“([^”]+)”|\\\"([^\\\"]+)\\\"\", text)\n",
        "    quotes = [\"\".join(q) for q in quotes]\n",
        "    expert_markers = re.compile(r\"\\b(Dr\\.|Professor|Prof\\.|PhD|MD|M\\.D\\.)\\b\")\n",
        "    return sum(1 for q in quotes if expert_markers.search(q))\n",
        "\n",
        "def _clickbait_score_from_title(title):\n",
        "    \"\"\"Lightweight score 0..1 from clickbait terms/punctuation.\"\"\"\n",
        "    if not title: return 0.0\n",
        "    title_l = title.lower()\n",
        "    pats = [\n",
        "        r\"you won'?t believe\", r\"what happened next\", r\"shocking\", r\"omg\", r\"wow\",\n",
        "        r\"destroys\", r\"obliterates\", r\"leaked\", r\"exposed\", r\"secret\", r\"no one is talking about\"\n",
        "    ]\n",
        "    hits = sum(1 for p in pats if re.search(p, title_l))\n",
        "    excls = title.count(\"!\")\n",
        "    return min(1.0, 0.15*hits + 0.05*excls)\n",
        "\n",
        "def extract_article_features_from_url(url):\n",
        "    \"\"\"\n",
        "    Fetch `url` and auto-extract a best-effort feature dict compatible with `score_articles`.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        html = _fetch_html(url)\n",
        "    except Exception as e:\n",
        "        # Return a minimal fallback with only URL and domain\n",
        "        d = urlparse(url).netloc\n",
        "        return {\n",
        "            \"title\": url, \"url\": url, \"domain\": d,\n",
        "            # leave other fields None/omitted; scorer handles defaults\n",
        "        }\n",
        "\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    domain = urlparse(url).netloc\n",
        "\n",
        "    # Title\n",
        "    title = None\n",
        "    if soup.title and soup.title.string:\n",
        "        title = soup.title.string.strip()\n",
        "    else:\n",
        "        ogt = soup.find(\"meta\", {\"property\": \"og:title\"})\n",
        "        title = _text_or_attr(ogt, \"content\") or url\n",
        "\n",
        "    # Dates\n",
        "    published_at = _extract_date(soup)\n",
        "\n",
        "    # Evidence signals\n",
        "    cites, primary = _count_outbound_links(soup, domain)\n",
        "    text_body = soup.get_text(\" \", strip=True)\n",
        "    experts = _count_named_experts(text_body)\n",
        "\n",
        "    # Transparency/quality/paywall\n",
        "    has_byline = _detect_byline(soup)\n",
        "    has_contact = _detect_contact_info(soup)\n",
        "    has_about = _detect_about_page(soup)\n",
        "    corrections = _detect_corrections_policy(soup)\n",
        "    is_press_rel = _detect_press_release(soup)\n",
        "    ads_density = _estimate_ads_density(soup)\n",
        "    paywalled = _detect_paywall(soup)\n",
        "\n",
        "    # Clickbait / opinion cues\n",
        "    clickbait_score = _clickbait_score_from_title(title)\n",
        "\n",
        "    # Outlet type guess (very rough)\n",
        "    outlet_type = _guess_outlet_type(domain)\n",
        "\n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"url\": url,\n",
        "        \"domain\": domain,\n",
        "        \"outlet_type\": outlet_type,\n",
        "        \"published_at\": published_at,\n",
        "        \"has_byline\": has_byline,\n",
        "        \"has_about_page\": has_about,\n",
        "        \"has_contact_info\": has_contact,\n",
        "        \"corrections_policy\": corrections,\n",
        "        \"cites_sources_count\": cites,\n",
        "        \"links_to_primary_sources\": primary,\n",
        "        \"quotes_named_experts\": experts,\n",
        "        \"advertising_density\": ads_density,\n",
        "        \"is_press_release\": is_press_rel,\n",
        "        \"clickbait_score\": clickbait_score,\n",
        "        \"paywalled\": paywalled,\n",
        "        # subjectivity_score & polarity_score left None by default\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# PUBLIC API: pass only URLs (strings/list)\n",
        "# ==========================================\n",
        "def score_articles_from_urls(urls):\n",
        "    \"\"\"\n",
        "    Accepts a single URL string or a list of URL strings.\n",
        "    Returns scored articles (same structure as score_articles output).\n",
        "    \"\"\"\n",
        "    if isinstance(urls, str):\n",
        "        urls = [urls]\n",
        "    articles = [extract_article_features_from_url(u) for u in urls]\n",
        "    return score_articles(articles)\n",
        "\n",
        "# ====================\n",
        "# Example (quick test)\n",
        "# ====================\n",
        "if __name__ == \"__main__\":\n",
        "    test_url = \"https://www.university.edu/news/2025/09/air-quality-asthma-study\"\n",
        "    result = score_articles_from_urls(test_url)[0]\n",
        "    print(\"Title:\", result[\"title\"])\n",
        "    print(\"Credibility:\", result[\"credibility_score_0_100\"])\n",
        "    print(\"Objectivity:\", result[\"objectivity_stars_0_5\"])\n",
        "    print(\"Overall:\", result[\"overall_stars_0_5\"])\n",
        "    print(\"Rationale:\", result[\"rationale\"])\n",
        "    print(\"Breakdown:\", result[\"breakdown\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTO7oF4QJ0a-",
        "outputId": "d9f7b667-922b-4c2c-bc41-5ac13b078a76"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: https://www.university.edu/news/2025/09/air-quality-asthma-study\n",
            "Credibility: 48.4\n",
            "Objectivity: 2.8\n",
            "Overall: 2.54\n",
            "Rationale: domain=0.85, evidence=0.00, transparency=0.30, quality=0.70, recency=0.50, objectivity=0.70, penalties=0.00.\n",
            "Breakdown: {'domain': 0.85, 'evidence': 0.0, 'transparency': 0.3, 'quality': 0.7, 'recency': 0.5, 'objectivity': 0.7, 'penalties': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example URL (you can swap this for any article you want to test)\n",
        "url = \"https://apnews.com/article/charlie-kirk-shooting-political-violence-reaction-87f6755421938d0a0d7c5905be10767f\"\n",
        "\n",
        "# Run the scoring\n",
        "result = score_articles_from_urls(url)[0]\n",
        "\n",
        "# Show results\n",
        "print(\"Title:\", result[\"title\"])\n",
        "print(\"Credibility Score (0–100):\", result[\"credibility_score_0_100\"])\n",
        "print(\"Objectivity Stars (0–5):\", result[\"objectivity_stars_0_5\"])\n",
        "print(\"Overall Stars (0–5):\", result[\"overall_stars_0_5\"])\n",
        "print(\"Rationale:\", result[\"rationale\"])\n",
        "print(\"Breakdown:\", result[\"breakdown\"])\n"
      ],
      "metadata": {
        "id": "ksQS3RTGKH5y",
        "outputId": "10a12e3c-3180-4020-c3eb-eedc8d0bba00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Charlie Kirk shooting brings condemnation from victims of political violence | AP News\n",
            "Credibility Score (0–100): 56.4\n",
            "Objectivity Stars (0–5): 3.25\n",
            "Overall Stars (0–5): 2.95\n",
            "Rationale: domain=0.50, evidence=0.45, transparency=0.90, quality=0.55, recency=0.30, objectivity=0.70, penalties=0.00.\n",
            "Breakdown: {'domain': 0.5, 'evidence': 0.45, 'transparency': 0.9, 'quality': 0.55, 'recency': 0.3, 'objectivity': 0.7, 'penalties': 0.0}\n"
          ]
        }
      ]
    }
  ]
}